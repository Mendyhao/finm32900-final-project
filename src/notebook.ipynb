{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replicating Corporate Bond Columns from “Evidence from Many Asset Classes”\n",
    "\n",
    "This notebook will give a brief tour of our cleaned data and some of the analysis performed in the code. \n",
    "\n",
    "We first load the required packages needed to load the data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Derive current working directory\n",
    "current_dir = Path(os.getcwd())\n",
    "\n",
    "# If current working directory is not 'src', update it\n",
    "if current_dir.stem != 'src':\n",
    "    src_directory = current_dir / 'src'\n",
    "    os.chdir(src_directory)\n",
    "    \n",
    "import config\n",
    "\n",
    "OUTPUT_DIR = Path(config.OUTPUT_DIR)\n",
    "DATA_DIR = Path(config.DATA_DIR)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We automate collection of TRACE data by connecting to the WRDS database and then export the data as a .csv file. Lehman Brothers dataset is downloaded manually from the following link:\n",
    "http://www.columbia.edu/acis/eds/holdings/1021/\n",
    "\n",
    "Mergent FISD/NAIC is also downloaded manually from WRDS as the wrds package in Python does not have full access to the variables in the table \"naic_bond_transactions\" of \"fisd\" libarary:\n",
    "https://wrds-www.wharton.upenn.edu/data-dictionary/fisd_naic/naic_bond_transactions/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading library list...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import wrds\n",
    "WRDS_USERNAME = config.WRDS_USERNAME\n",
    "\n",
    "# Connect to WRDS\n",
    "db = wrds.Connection(wrds_username=WRDS_USERNAME)\n",
    "\n",
    "# Collect TRACE data\n",
    "sql_query_T = \"\"\"select date,cusip,price_l5m,coupon,yield,maturity\n",
    "                        from wrdsapps.bondret \n",
    "                        \"\"\"\n",
    "df_T = db.raw_sql(sql_query_T)\n",
    "\n",
    "# Export output\n",
    "df_T.to_csv(DATA_DIR / \"TRACE.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then load the Lehman Brothers dataset along with the Mergent FISD/NAIC dataset. Regular expression patterns are used to read the Lehman datasets into columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "folder_path = DATA_DIR / 'manual/Lehman data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define regular expression pattern, only match needed columns\n",
    "pattern = re.compile(\n",
    "    r'(\\S{8})\\s+'          # cusip\n",
    "    r'.*?\\s{2,}'           # skip name column\n",
    "    r'(\\d{8})\\s+'          # date\n",
    "    r'.*?\\s+'              # skip idate \n",
    "    r'(\\d{8})\\s+'          # mdate\n",
    "    r'.*?\\s+'              # skip tdrmtx column\n",
    "    r'(-?\\d+\\.\\d{3})\\s+'   # fprc\n",
    "    r'.*?\\s+'              # skip aint column\n",
    "    r'(-?\\d+\\.\\d{4})\\s+'   # cp\n",
    "    r'(-?\\d+\\.\\d{3})\\s+'   # yld\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then move to processing the dataframe to get the information we need. This is still part of the same function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(folder_path)\n",
    "dfs = []\n",
    "\n",
    "for file in files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    data = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            match = pattern.match(line)\n",
    "            if match:\n",
    "                # extract interested columns only\n",
    "                data.append(match.groups())\n",
    "                \n",
    "    # specify columns\n",
    "    columns = ['cusip', 'date', 'maturity', 'fprc', 'cp', 'yld']\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    dfs.append(df)\n",
    "\n",
    "# concatenate all dataframes into a large dataframe\n",
    "dfL = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "\n",
    "# convert date format\n",
    "dfL['date'] = pd.to_datetime(dfL['date'], format='%Y%m%d', errors='coerce')\n",
    "dfL['maturity'] = pd.to_datetime(dfL['maturity'], format='%Y%m%d', errors='coerce')\n",
    "dfL = dfL.dropna(subset=['maturity'])\n",
    "\n",
    "# convert numbers to numeric format\n",
    "convert_float = ['fprc', 'cp', 'yld']\n",
    "dfL[convert_float] = dfL[convert_float].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Calculate month_to_maturity \n",
    "dfL['month_to_maturity'] = (dfL['maturity'].dt.to_period('M') - dfL['date'].dt.to_period('M')).apply(lambda x: x.n)\n",
    "\n",
    "dfL = dfL[dfL['month_to_maturity'] <= 360]\n",
    "\n",
    "\n",
    "stdL = ['id', 'date', 'maturity', 'price', 'coupon', 'yield', 'month_to_maturity']\n",
    "dfL = dfL.rename(columns=dict(zip(dfL.columns, stdL)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading the Lehman Brothers data, we load the TRACE dataset we initially collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) TRACE\n",
    "file_path_T = DATA_DIR / 'TRACE.csv'\n",
    "dfT = pd.read_csv(file_path_T)\n",
    "dfT['yield'] = dfT['yield']*100 # data automatically collected\n",
    "\n",
    "stdT = ['date', 'id', 'price', 'coupon', 'yield', 'maturity']\n",
    "dfT = dfT.rename(columns=dict(zip(dfT.columns, stdT)))\n",
    "dfT['date'] = pd.to_datetime(dfT['date'], format='%Y-%m-%d')\n",
    "dfT['maturity'] = pd.to_datetime(dfT['maturity'], format='%Y-%m-%d')\n",
    "\n",
    "# dfT['yield'] = dfT['yield'].str.rstrip('%')\n",
    "convert_float = ['yield', 'coupon', 'price']\n",
    "dfT[convert_float] = dfT[convert_float].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Calculate month_to_maturity\n",
    "dfT['month_to_maturity'] = (dfT['maturity'].dt.to_period('M') - dfT['date'].dt.to_period('M')).apply(lambda x: x.n)\n",
    "dfT = dfT[dfT['month_to_maturity'] <= 360]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then load the Mergent dataset, change the datatypes of specific columns, and only keep the rows where the day is the last day in each month, as specified in the paper. We then calculate month_to_maturity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Mergent\n",
    "file_path_M = DATA_DIR / 'manual' / 'Mergent_part.csv'\n",
    "dfM = pd.read_csv(file_path_M)\n",
    "\n",
    "# Rename columns\n",
    "stdM = ['id', 'price', 'coupon', 'date', 'maturity', 'yield']\n",
    "dfM = dfM.rename(columns=dict(zip(dfM.columns, stdM)))\n",
    "\n",
    "\n",
    "# Change data types\n",
    "convert_float = ['coupon', 'price', 'yield']\n",
    "dfM[convert_float] = dfM[convert_float].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Deal with \"date\"\n",
    "dfM['date'] = pd.to_datetime(dfM['date'], format='%Y-%m-%d', errors = 'coerce')\n",
    "dfM['maturity'] = pd.to_datetime(dfM['maturity'], format='%Y-%m-%d', errors = 'coerce')\n",
    "\n",
    "# Filter useful rows in Mergent dataset\n",
    "# start_date = pd.Timestamp('1998-04-01')\n",
    "# end_date = pd.Timestamp('2002-06-30')\n",
    "# dfM = dfM[(dfM['date'] >= start_date) & (dfM['date'] <= end_date)]\n",
    "\n",
    "# Only keep rows where the day is the latest in each month\n",
    "dfM = dfM.groupby([dfM['id'], dfM['date'].dt.year, dfM['date'].dt.month]).apply(lambda x: x.loc[x['date'].idxmax()])\n",
    "dfM = dfM.reset_index(drop=True)\n",
    "dfM = dfM.dropna(subset=['maturity'])\n",
    "\n",
    "# Calculate month_to_maturity\n",
    "dfM['month_to_maturity'] = (dfM['maturity'].dt.to_period('M') - dfM['date'].dt.to_period('M')).apply(lambda x: x.n)\n",
    "dfM = dfM[dfM['month_to_maturity'] <= 360]\n",
    "# dfM['maturity'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then merge the three datasets together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge 1) & 2) & 3)\n",
    "df_merge = pd.concat([dfL, dfT, dfM], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After merging the three datasets into one, we clean the merged data. The cleaning process follows Nozawa (2017). First, corporate bond prices that are lower than one cent is removed. Second, adjacent prices whose product is less than -0.04 are also removed because this may indicate there is a recording error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Drop corporate price below on cent per dollar\n",
    "df_drop = df_merge[~(df_merge['price'] < 0.01)]\n",
    "\n",
    "# 2. Remove rows of adjacent returns whose product is less than -0.04\n",
    "# Calculate return\n",
    "df_sorted = df_drop.sort_values('date', ascending=True).reset_index(drop=True)\n",
    "df_sorted['date'].is_monotonic_increasing\n",
    "grouped = df_sorted.groupby('id')\n",
    "df_sorted['return'] = grouped.apply(lambda x:(x['price'] + x['coupon']) / x['price'].shift(1)).reset_index(level=0, drop=True)\n",
    "\n",
    "# Remove rows of adjacent returns whose product is less than -0.04\n",
    "df_b = df_sorted.sort_values(['id', 'date'])\n",
    "indices_to_remove = []\n",
    "\n",
    "# The following code is commented out because previous running result shows that there is no such case\n",
    "# You can check by bringing these code back\n",
    "\n",
    "# for _, group in df_b.groupby('id'):\n",
    "#     product = group['return'].shift(1) * group['return']\n",
    "#     mask = product < -0.04\n",
    "#     indices_to_remove.extend(group.loc[mask].index)\n",
    "\n",
    "df_remove = df_b.drop(indices_to_remove)\n",
    "df_b = df_remove.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we construct interpolated risk-free rate based on constant-maturity Treasury yields to merge into our data. First, NaN values are filled using linear interpolation method, then we derive interpolated risk-free rates for maturities every month during 1 month to 360 months using the cubic splines method. We export this as a .csv file to then merge into our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import CubicSpline\n",
    "\n",
    "file_path = DATA_DIR / 'manual' /'Monthly Treasury Yield.csv'\n",
    "\n",
    "# Read monthly T-bill interest rates\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "convert_numeric = ['M01', 'M03', 'M06', 'Y01', 'Y02', 'Y03', 'Y05', 'Y07', 'Y10', 'Y20', 'Y30']\n",
    "for column in convert_numeric:\n",
    "    df[column] = pd.to_numeric(df[column], errors='coerce')\n",
    "\n",
    "# Interpolate df to get rf in each month\n",
    "df[convert_numeric] = df[convert_numeric].interpolate(method='linear', axis=1, limit_direction='both')\n",
    "\n",
    "\n",
    "# Initialize an empty list to store interpolated results\n",
    "interpolated_results = []\n",
    "\n",
    "periods = ['M01', 'M03', 'M06', 'Y01', 'Y02', 'Y03', 'Y05', 'Y07', 'Y10', 'Y20', 'Y30']  # maturities\n",
    "\n",
    "# Define numeric representations for maturities (in year)\n",
    "maturity_numeric = np.array([1/12, 3/12, 6/12, 1, 2, 3, 5, 7, 10, 20, 30])\n",
    "\n",
    "# Loop through every row to interpolate yield rate\n",
    "for index, row in df.iterrows():\n",
    "    \n",
    "    # Extract every row and corresponding columns\n",
    "    monthly_data = df.loc[index, periods].values\n",
    "    \n",
    "    # Create cubic splines interpolation function\n",
    "    cs = CubicSpline(maturity_numeric, monthly_data)\n",
    "\n",
    "    # Create total number of maturities after interpolation\n",
    "    maturity_interpolate = np.linspace(maturity_numeric.min(), maturity_numeric.max(), 360)\n",
    "\n",
    "    # Derive the interpolation results\n",
    "    interpolated_rates = cs(maturity_interpolate)\n",
    "    \n",
    "    interpolated_results.append(interpolated_rates)\n",
    "\n",
    "# Turn into a dataframe\n",
    "interpolated_results = pd.DataFrame(interpolated_results)\n",
    "\n",
    "# Rename columns\n",
    "new_columns = [f'M{i+1}' for i in range(360)]\n",
    "interpolated_results.columns = new_columns\n",
    "\n",
    "# Create a series of dates corresponding to the results\n",
    "date_range = pd.date_range(start='1953/4/1', end='2024/1/1', freq='MS')\n",
    "date_df = pd.DataFrame(date_range[:len(interpolated_results)], columns=['Date'])\n",
    "\n",
    "# Concatenate the dates with 'zero_rate'\n",
    "interpolated_results = pd.DataFrame(pd.concat([date_df, interpolated_results], axis=1))\n",
    "\n",
    "interpolated_results.to_csv(OUTPUT_DIR / \"Interpolated_Rf.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then load the interpolated risk-free rate, change the data to long format, and calculate the yield spread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_b['year_month'] = df_b['date'].dt.to_period('M')\n",
    "\n",
    "# Load interpolated rf rate\n",
    "rf_rates_df = pd.read_csv(OUTPUT_DIR / 'Interpolated_Rf.csv')\n",
    "rf_rates_df['Date'] = pd.to_datetime(rf_rates_df['Date'])\n",
    "rf_rates_df['year_month'] = rf_rates_df['Date'].dt.to_period('M')\n",
    "\n",
    "# Change rf to long format\n",
    "rf_long_df = rf_rates_df.melt(id_vars=['Date', 'year_month'], var_name='month', value_name='rf_rate')\n",
    "rf_long_df['month_to_maturity'] = rf_long_df['month'].str.replace('M', '').astype(int)\n",
    "\n",
    "# Merge df_b and rf based on year_month and month_to_maturity\n",
    "merged_df = df_b.merge(rf_long_df, on=['year_month', 'month_to_maturity'], how='left')\n",
    "\n",
    "merged_df['excess_return'] = np.log(merged_df['return']) - np.log(merged_df['rf_rate']/100+1)\n",
    "merged_df = merged_df.dropna(subset=['excess_return'])\n",
    "\n",
    "# Calculate yield spread\n",
    "merged_df['yield_spread'] = merged_df['yield'] - merged_df['rf_rate']\n",
    "\n",
    "df_minus = merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After merging these datasets together, we then replicate the corporate bond columns. We sort portfolios by yield spread, then calculate the average value of excess return for each group to derive the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_date = datetime(2023, 12, 31)\n",
    "df_sum = df_minus[df_minus['date']<=end_date]\n",
    "\n",
    "df_sum = df_sum.dropna(subset=['yield_spread'])\n",
    "df_sum['date'] = df_sum['date'].dt.to_period(\"M\")\n",
    "\n",
    "# Sort portfolios by yield_spread\n",
    "df_sum['group'] = df_sum.groupby('date')['yield_spread'].transform(lambda x: pd.qcut(x, 10, labels=False, duplicates='drop'))\n",
    "\n",
    "# Calculate average value of excess_return for each group\n",
    "grouped = df_sum.groupby(['date', 'group'])['excess_return'].mean().reset_index()\n",
    "\n",
    "# Derive result\n",
    "result = grouped.pivot(index='date', columns='group', values='excess_return')\n",
    "\n",
    "result = result.reset_index()\n",
    "\n",
    "# Rename the columns\n",
    "rename = result.columns[1:]\n",
    "new_column_names = ['US_bonds_{:02d}'.format(i+11) for i in range(len(rename))]\n",
    "columns_mapping = dict(zip(rename, new_column_names))\n",
    "result.rename(columns=columns_mapping, inplace=True)\n",
    "\n",
    "result.to_csv(OUTPUT_DIR / 'Corporate Bond Return Replicated.csv', index=False)  # export output to specified path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
