{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replicating Corporate Bond Columns from “Evidence from Many Asset Classes”\n",
    "\n",
    "This notebook will give a brief tour of our cleaned data and some of the analysis performed in the code. \n",
    "\n",
    "We first load the required packages needed to load the data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "from pathlib import Path\n",
    "import os\n",
    "OUTPUT_DIR = Path(config.OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We automate collection of TRACE data by connecting to the WRDS database and then export the data as a .csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wrds\n",
    "\n",
    "def collect_trace():\n",
    "    # Connect to WRDS\n",
    "    db = wrds.Connection(wrds_username=WRDS_USERNAME)\n",
    "\n",
    "    # Collect TRACE data\n",
    "    sql_query_T = \"\"\"select date,cusip,price_l5m,coupon,yield,maturity\n",
    "                            from wrdsapps.bondret \n",
    "                            \"\"\"\n",
    "    df_T = db.raw_sql(sql_query_T)\n",
    "\n",
    "    return df_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_T = collect_trace()\n",
    "\n",
    "# Export output\n",
    "df_T.to_csv(DATA_DIR / \"TRACE.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then load the Lehman Brothers dataset along with the Mergent FISD/NAIC dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def combine_Lehman():\n",
    "\n",
    "    folder_path = DATA_DIR / 'manual/Lehman data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use regular expression patterns to collect only the columns we need from the Lehman Brothers data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Define regular expression pattern, only match needed columns\n",
    "    pattern = re.compile(\n",
    "        r'(\\S{8})\\s+'          # cusip\n",
    "        r'.*?\\s{2,}'           # skip name column\n",
    "        r'(\\d{8})\\s+'          # date\n",
    "        r'.*?\\s+'              # skip idate \n",
    "        r'(\\d{8})\\s+'          # mdate\n",
    "        r'.*?\\s+'              # skip tdrmtx column\n",
    "        r'(-?\\d+\\.\\d{3})\\s+'   # fprc\n",
    "        r'.*?\\s+'              # skip aint column\n",
    "        r'(-?\\d+\\.\\d{4})\\s+'   # cp\n",
    "        r'(-?\\d+\\.\\d{3})\\s+'   # yld\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then move to processing the dataframe to get the information we need. This is still part of the same function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    files = os.listdir(folder_path)\n",
    "    dfs = []\n",
    "\n",
    "    for file in files:\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        data = []\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                match = pattern.match(line)\n",
    "                if match:\n",
    "                    # extract interested columns only\n",
    "                    data.append(match.groups())\n",
    "                    \n",
    "        # specify columns\n",
    "        columns = ['cusip', 'date', 'maturity', 'fprc', 'cp', 'yld']\n",
    "        df = pd.DataFrame(data, columns=columns)\n",
    "        dfs.append(df)\n",
    "\n",
    "    # concatenate all dataframes into a large dataframe\n",
    "    dfL = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "\n",
    "    # convert date format\n",
    "    dfL['date'] = pd.to_datetime(dfL['date'], format='%Y%m%d', errors='coerce')\n",
    "    dfL['maturity'] = pd.to_datetime(dfL['maturity'], format='%Y%m%d', errors='coerce')\n",
    "    dfL = dfL.dropna(subset=['maturity'])\n",
    "\n",
    "    # convert numbers to numeric format\n",
    "    convert_float = ['fprc', 'cp', 'yld']\n",
    "    dfL[convert_float] = dfL[convert_float].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # Calculate month_to_maturity \n",
    "    dfL['month_to_maturity'] = (dfL['maturity'].dt.to_period('M') - dfL['date'].dt.to_period('M')).apply(lambda x: x.n)\n",
    "    \n",
    "    dfL = dfL[dfL['month_to_maturity'] <= 360]\n",
    "\n",
    "\n",
    "    stdL = ['id', 'date', 'maturity', 'price', 'coupon', 'yield', 'month_to_maturity']\n",
    "    dfL = dfL.rename(columns=dict(zip(dfL.columns, stdL)))\n",
    "    \n",
    "    return dfL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading the Lehman Brothers data, we load the TRACE dataset we initially collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_trace():\n",
    "    # 2) TRACE\n",
    "    file_path_T = DATA_DIR / 'TRACE.csv'\n",
    "    dfT = pd.read_csv(file_path_T)\n",
    "    dfT['yield'] = dfT['yield']*100 # data automatically collected\n",
    "\n",
    "    stdT = ['date', 'id', 'price', 'coupon', 'yield', 'maturity']\n",
    "    dfT = dfT.rename(columns=dict(zip(dfT.columns, stdT)))\n",
    "    dfT['date'] = pd.to_datetime(dfT['date'], format='%Y-%m-%d')\n",
    "    dfT['maturity'] = pd.to_datetime(dfT['maturity'], format='%Y-%m-%d')\n",
    "\n",
    "    convert_float = ['yield', 'coupon', 'price']\n",
    "    dfT[convert_float] = dfT[convert_float].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # Calculate month_to_maturity\n",
    "    dfT['month_to_maturity'] = (dfT['maturity'].dt.to_period('M') - dfT['date'].dt.to_period('M')).apply(lambda x: x.n)\n",
    "    dfT = dfT[dfT['month_to_maturity'] <= 360]\n",
    "\n",
    "\n",
    "    return dfT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then load the Mergent dataset, change the datatypes of specific columns, and only keep the rows where the day is the last day in each month, as specified in the paper. We then calculate month_to_maturity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_mergent():\n",
    "    # 3) Mergent\n",
    "    file_path_M = DATA_DIR / 'manual' / 'Mergent_part.csv'\n",
    "    dfM = pd.read_csv(file_path_M)\n",
    "    \n",
    "    # Rename columns\n",
    "    stdM = ['id', 'price', 'coupon', 'date', 'maturity', 'yield']\n",
    "    dfM = dfM.rename(columns=dict(zip(dfM.columns, stdM)))\n",
    "    \n",
    "\n",
    "    # Change data types\n",
    "    convert_float = ['coupon', 'price', 'yield']\n",
    "    dfM[convert_float] = dfM[convert_float].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # Deal with \"date\"\n",
    "    dfM['date'] = pd.to_datetime(dfM['date'], format='%Y-%m-%d', errors = 'coerce')\n",
    "    dfM['maturity'] = pd.to_datetime(dfM['maturity'], format='%Y-%m-%d', errors = 'coerce')\n",
    "\n",
    "    # Only keep rows where the day is the latest in each month\n",
    "    dfM = dfM.groupby([dfM['id'], dfM['date'].dt.year, dfM['date'].dt.month]).apply(lambda x: x.loc[x['date'].idxmax()])\n",
    "    dfM = dfM.reset_index(drop=True)\n",
    "    dfM = dfM.dropna(subset=['maturity'])\n",
    "\n",
    "    # Calculate month_to_maturity\n",
    "    dfM['month_to_maturity'] = (dfM['maturity'].dt.to_period('M') - dfM['date'].dt.to_period('M')).apply(lambda x: x.n)\n",
    "    dfM = dfM[dfM['month_to_maturity'] <= 360]\n",
    "    # dfM['maturity'].isna().sum()\n",
    "\n",
    "    return dfM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then merge the three datasets together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_and_fillna(dfL, dfT, dfM):\n",
    "    # Merge 1) & 2) & 3)\n",
    "    df_merge = pd.concat([dfL, dfT, dfM], axis=0)\n",
    "\n",
    "    return df_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After merging the three datasets into one, we clean the merged data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(df_merge):\n",
    "    \n",
    "    # 1. Drop corporate price below on cent per dollar\n",
    "    df_drop = df_merge[~(df_merge['price'] < 0.01)]\n",
    "    \n",
    "    # 2. Remove rows of adjacent returns whose product is less than -0.04\n",
    "    # Calculate return\n",
    "    df_sorted = df_drop.sort_values('date', ascending=True).reset_index(drop=True)\n",
    "    df_sorted['date'].is_monotonic_increasing\n",
    "    grouped = df_sorted.groupby('id')\n",
    "    df_sorted['return'] = grouped.apply(lambda x:(x['price'] + x['coupon']) / x['price'].shift(1)).reset_index(level=0, drop=True)\n",
    "\n",
    "    # Remove rows of adjacent returns whose product is less than -0.04\n",
    "    df_b = df_sorted.sort_values(['id', 'date'])\n",
    "    indices_to_remove = []\n",
    "\n",
    "    df_remove = df_b.drop(indices_to_remove)\n",
    "    df_b = df_remove.reset_index(drop=True)\n",
    "\n",
    "    return df_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we construct interpolated risk-free rate based on constant-maturity Treasury yields to merge into our data. First, NaN values are filled using linear interpolation method, then we derive interpolated risk-free rates for maturities every month during 1 month to 360 months using the cubic splines method. We export this as a .csv file to then merge into our data. Our method for creating the Treasury zero-coupon yield curve from 1992/7/1 to 2024/1/1 can be found in the Construction of Matching Treasury Bonds.py file in our repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_rf():\n",
    "    file_path = DATA_DIR / 'manual' /'Monthly Treasury Yield.csv'\n",
    "\n",
    "    # Read monthly T-bill interest rates\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "    convert_numeric = ['M01', 'M03', 'M06', 'Y01', 'Y02', 'Y03', 'Y05', 'Y07', 'Y10', 'Y20', 'Y30']\n",
    "    for column in convert_numeric:\n",
    "        df[column] = pd.to_numeric(df[column], errors='coerce')\n",
    "\n",
    "    # Interpolate df to get rf in each month\n",
    "    df[convert_numeric] = df[convert_numeric].interpolate(method='linear', axis=1, limit_direction='both')\n",
    "\n",
    "\n",
    "    # Initialize an empty list to store interpolated results\n",
    "    interpolated_results = []\n",
    "\n",
    "    periods = ['M01', 'M03', 'M06', 'Y01', 'Y02', 'Y03', 'Y05', 'Y07', 'Y10', 'Y20', 'Y30']  # maturities\n",
    "\n",
    "    # Define numeric representations for maturities (in year)\n",
    "    maturity_numeric = np.array([1/12, 3/12, 6/12, 1, 2, 3, 5, 7, 10, 20, 30])\n",
    "\n",
    "    # Loop through every row to interpolate yield rate\n",
    "    for index, row in df.iterrows():\n",
    "        \n",
    "        # Extract every row and corresponding columns\n",
    "        monthly_data = df.loc[index, periods].values\n",
    "        \n",
    "        # Create cubic splines interpolation function\n",
    "        cs = CubicSpline(maturity_numeric, monthly_data)\n",
    "\n",
    "        # Create total number of maturities after interpolation\n",
    "        maturity_interpolate = np.linspace(maturity_numeric.min(), maturity_numeric.max(), 360)\n",
    "\n",
    "        # Derive the interpolation results\n",
    "        interpolated_rates = cs(maturity_interpolate)\n",
    "        \n",
    "        interpolated_results.append(interpolated_rates)\n",
    "\n",
    "    # Turn into a dataframe\n",
    "    interpolated_results = pd.DataFrame(interpolated_results)\n",
    "\n",
    "    # Rename columns\n",
    "    new_columns = [f'M{i+1}' for i in range(360)]\n",
    "    interpolated_results.columns = new_columns\n",
    "\n",
    "    # Create a series of dates corresponding to the results\n",
    "    date_range = pd.date_range(start='1953/4/1', end='2024/1/1', freq='MS')\n",
    "    date_df = pd.DataFrame(date_range[:len(interpolated_results)], columns=['Date'])\n",
    "\n",
    "    # Concatenate the dates with 'zero_rate'\n",
    "    interpolated_results = pd.DataFrame(pd.concat([date_df, interpolated_results], axis=1))\n",
    "\n",
    "    return interpolated_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then load the interpolated risk-free rate, change the data to long format, and calculate the yield spread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minus_rf(df_b):\n",
    "\n",
    "    df_b['year_month'] = df_b['date'].dt.to_period('M')\n",
    "\n",
    "    # Load interpolated rf rate\n",
    "    rf_rates_df = pd.read_csv(OUTPUT_DIR / 'Interpolated_Rf.csv')\n",
    "    rf_rates_df['Date'] = pd.to_datetime(rf_rates_df['Date'])\n",
    "    rf_rates_df['year_month'] = rf_rates_df['Date'].dt.to_period('M')\n",
    "\n",
    "    # Change rf to long format\n",
    "    rf_long_df = rf_rates_df.melt(id_vars=['Date', 'year_month'], var_name='month', value_name='rf_rate')\n",
    "    rf_long_df['month_to_maturity'] = rf_long_df['month'].str.replace('M', '').astype(int)\n",
    "\n",
    "    # Merge df_b and rf based on year_month and month_to_maturity\n",
    "    merged_df = df_b.merge(rf_long_df, on=['year_month', 'month_to_maturity'], how='left')\n",
    "\n",
    "    merged_df['excess_return'] = np.log(merged_df['return']) - np.log(merged_df['rf_rate']/100+1)\n",
    "    merged_df = merged_df.dropna(subset=['excess_return'])\n",
    "\n",
    "    # Calculate yield spread\n",
    "    merged_df['yield_spread'] = merged_df['yield'] - merged_df['rf_rate']\n",
    "\n",
    "    df_minus = merged_df\n",
    "\n",
    "    return df_minus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After merging these datasets together, we then replicate the corporate bond columns. We sort portfolios by yield spread, then calculate the average value of excess return for each group to derive the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replicate_columns(df_minus, end_date):\n",
    "    \n",
    "    df_sum = df_minus[df_minus['date']<=end_date]\n",
    "\n",
    "    df_sum = df_sum.dropna(subset=['yield_spread'])\n",
    "    df_sum['date'] = df_sum['date'].dt.to_period(\"M\")\n",
    "\n",
    "    # Sort portfolios by yield_spread\n",
    "    df_sum['group'] = df_sum.groupby('date')['yield_spread'].transform(lambda x: pd.qcut(x, 10, labels=False, duplicates='drop'))\n",
    "\n",
    "    # Calculate average value of excess_return for each group\n",
    "    grouped = df_sum.groupby(['date', 'group'])['excess_return'].mean().reset_index()\n",
    "\n",
    "    # Derive result\n",
    "    result = grouped.pivot(index='date', columns='group', values='excess_return')\n",
    "\n",
    "    result = result.reset_index()\n",
    "    \n",
    "    # Rename the columns\n",
    "    rename = result.columns[1:]\n",
    "    new_column_names = ['US_bonds_{:02d}'.format(i+11) for i in range(len(rename))]\n",
    "    columns_mapping = dict(zip(rename, new_column_names))\n",
    "    result.rename(columns=columns_mapping, inplace=True)\n",
    "\n",
    "    return result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
